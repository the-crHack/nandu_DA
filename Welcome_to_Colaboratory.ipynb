{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/the-crHack/nandu_DA/blob/main/Welcome_to_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyreadstat\n",
        "!pip install pandas pyreadstat python-docx\n",
        "!pip install pingouin"
      ],
      "metadata": {
        "id": "YPAs4NKtBL_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n"
      ],
      "metadata": {
        "id": "lJLEVXPaEccX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyreadstat\n",
        "\n",
        "# Load the .sav file\n",
        "sav_file_path = \"/content/DISSERTATION SPSS DATA.sav\"  # Update with your file path\n",
        "df, meta = pyreadstat.read_sav(sav_file_path)\n",
        "\n",
        "# Display basic info about the dataset\n",
        "print(\"Dataset Info:\")\n",
        "print(df.info())\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst 5 Rows of Data:\")\n",
        "print(df.head())\n",
        "\n",
        "# Display variable labels\n",
        "print(\"\\nVariable Labels:\")\n",
        "for var_name, var_label in zip(meta.column_names, meta.column_labels):\n",
        "    print(f\"{var_name}: {var_label}\")\n",
        "\n",
        "# Descriptive Statistics\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Checking missing values\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Correlation Matrix\n",
        "print(\"\\nCorrelation Matrix:\")\n",
        "print(df.corr())\n",
        "\n",
        "# Reliability Analysis (Cronbach's Alpha)\n",
        "from pingouin import cronbach_alpha\n",
        "\n",
        "alpha, _ = cronbach_alpha(data=df, ci=True)\n",
        "print(f\"\\nCronbach's Alpha: {alpha:.3f}\")\n",
        "\n",
        "# Save the processed data to CSV (optional)\n",
        "df.to_csv(\"processed_data.csv\", index=False)\n",
        "print(\"\\nProcessed data saved to 'processed_data.csv'.\")\n"
      ],
      "metadata": {
        "id": "_AvRxBdnBMCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the .sav file\n",
        "sav_file_path = \"/content/DISSERTATION SPSS DATA.sav\"  # Update with your file path\n",
        "df, meta = pyreadstat.read_sav(sav_file_path)\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Create a new Word document\n",
        "doc = Document()\n",
        "doc.add_heading(\"Analysis Report\", level=1)\n",
        "\n",
        "# Outer Loadings Section (Example using Mean)\n",
        "doc.add_heading(\"Outer Loadings\", level=2)\n",
        "for col in df.columns:\n",
        "    if df[col].dtype in ['float64', 'int64']:  # Ensure numeric columns only\n",
        "        doc.add_paragraph(f\"{col}: {df[col].mean():.3f}\")  # Example using mean\n",
        "\n",
        "# Reliability and Validity (Example using correlation)\n",
        "doc.add_heading(\"Reliability and Validity\", level=2)\n",
        "doc.add_paragraph(f\"Cronbach's Alpha: {df.corr().mean().mean():.3f}\")  # Example\n",
        "\n",
        "# Discriminant Validity – HTMT Matrix (Using Correlation)\n",
        "doc.add_heading(\"Discriminant Validity – HTMT Matrix\", level=2)\n",
        "corr_matrix = df.corr()\n",
        "for col1 in df.columns:\n",
        "    for col2 in df.columns:\n",
        "        if col1 != col2:  # Avoid self-correlation\n",
        "            doc.add_paragraph(f\"{col1} - {col2}: {corr_matrix[col1][col2]:.3f}\")\n",
        "\n",
        "# Collinearity Statistics (Variance Inflation Factor)\n",
        "doc.add_heading(\"Collinearity Statistics\", level=2)\n",
        "\n",
        "# Handle missing data before calculating VIF\n",
        "df_clean = df.fillna(df.mean())  # Replace NaNs with mean values\n",
        "\n",
        "# Compute VIF\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Variable\"] = df_clean.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(df_clean.values, i) for i in range(df_clean.shape[1])]\n",
        "\n",
        "# Add VIF to the document\n",
        "for row in vif_data.itertuples():\n",
        "    doc.add_paragraph(f\"{row.Variable}: {row.VIF:.3f}\")\n",
        "\n",
        "# Hypothesis Testing - Placeholder (Replace with actual calculations)\n",
        "doc.add_heading(\"Hypothesis Testing\", level=2)\n",
        "doc.add_paragraph(\"Direct Effects\")\n",
        "doc.add_paragraph(\"Indirect Effects\")\n",
        "\n",
        "# Model Fit - Placeholder (Replace with actual calculations)\n",
        "doc.add_heading(\"Model Fit\", level=2)\n",
        "doc.add_paragraph(\"SRMR: 0.050\")  # Example static value\n",
        "\n",
        "# Save the document\n",
        "output_doc_path = \"Final_Analysis_Report.docx\"\n",
        "doc.save(output_doc_path)\n",
        "print(f\"Report saved as {output_doc_path}\")"
      ],
      "metadata": {
        "id": "mOfJWgacCCyO",
        "outputId": "d1f6825a-a7da-4598-946a-05705c56777e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report saved as Final_Analysis_Report.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyreadstat\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Load the .sav file\n",
        "sav_file_path = \"/content/DISSERTATION SPSS DATA.sav\"  # Update with your file path\n",
        "df, meta = pyreadstat.read_sav(sav_file_path)\n",
        "\n",
        "# Handle missing data before calculations\n",
        "df_clean = df.fillna(df.mean())  # Replace NaNs with mean values\n",
        "\n",
        "# Create a dictionary for structured output\n",
        "structured_data = {\n",
        "    \"Section\": [],\n",
        "    \"Metric\": [],\n",
        "    \"Value\": []\n",
        "}\n",
        "\n",
        "# Outer Loadings (Example using Mean)\n",
        "for col in df_clean.columns:\n",
        "    if df_clean[col].dtype in ['float64', 'int64']:  # Ensure numeric columns only\n",
        "        structured_data[\"Section\"].append(\"Outer Loadings\")\n",
        "        structured_data[\"Metric\"].append(col)\n",
        "        structured_data[\"Value\"].append(df_clean[col].mean())  # Example using mean\n",
        "\n",
        "# Reliability and Validity (Example using correlation)\n",
        "structured_data[\"Section\"].append(\"Reliability and Validity\")\n",
        "structured_data[\"Metric\"].append(\"Cronbach's Alpha\")\n",
        "structured_data[\"Value\"].append(df_clean.corr().mean().mean())  # Example\n",
        "\n",
        "# Discriminant Validity – HTMT Matrix (Using Correlation)\n",
        "corr_matrix = df_clean.corr()\n",
        "for col1 in df_clean.columns:\n",
        "    for col2 in df_clean.columns:\n",
        "        if col1 != col2:  # Avoid self-correlation\n",
        "            structured_data[\"Section\"].append(\"Discriminant Validity\")\n",
        "            structured_data[\"Metric\"].append(f\"{col1} - {col2}\")\n",
        "            structured_data[\"Value\"].append(corr_matrix[col1][col2])\n",
        "\n",
        "# Collinearity Statistics (Variance Inflation Factor)\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Variable\"] = df_clean.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(df_clean.values, i) for i in range(df_clean.shape[1])]\n",
        "\n",
        "for row in vif_data.itertuples():\n",
        "    structured_data[\"Section\"].append(\"Collinearity Statistics\")\n",
        "    structured_data[\"Metric\"].append(row.Variable)\n",
        "    structured_data[\"Value\"].append(row.VIF)\n",
        "\n",
        "# Hypothesis Testing - Placeholder (Replace with actual calculations)\n",
        "structured_data[\"Section\"].append(\"Hypothesis Testing\")\n",
        "structured_data[\"Metric\"].append(\"Direct Effects\")\n",
        "structured_data[\"Value\"].append(\"Placeholder\")\n",
        "\n",
        "structured_data[\"Section\"].append(\"Hypothesis Testing\")\n",
        "structured_data[\"Metric\"].append(\"Indirect Effects\")\n",
        "structured_data[\"Value\"].append(\"Placeholder\")\n",
        "\n",
        "# Model Fit - Placeholder (Replace with actual calculations)\n",
        "structured_data[\"Section\"].append(\"Model Fit\")\n",
        "structured_data[\"Metric\"].append(\"SRMR\")\n",
        "structured_data[\"Value\"].append(0.050)  # Example static value\n",
        "\n",
        "# Convert dictionary to DataFrame and save to CSV\n",
        "output_csv_path = \"Final_Analysis_Report.csv\"\n",
        "structured_df = pd.DataFrame(structured_data)\n",
        "structured_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"Report saved as {output_csv_path}\")\n"
      ],
      "metadata": {
        "id": "uK-uywP6DLYM",
        "outputId": "ead10be1-8fe2-4e2b-b1ec-53c98e71ba85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report saved as Final_Analysis_Report.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyreadstat\n",
        "\n",
        "# Step 1: Load the .sav file\n",
        "file_path = '/content/DISSERTATION SPSS DATA.sav'\n",
        "df, meta = pyreadstat.read_sav(file_path)\n",
        "\n",
        "# Step 3: Define the columns you want to extract\n",
        "columns_to_extract = [\n",
        "    'CSRS1', 'CSRS2', 'CSRS3', 'CSRS4', 'CSRS5', 'CSRS6', 'CSRS7',\n",
        "    'EL1', 'EL2', 'EL3', 'EL4', 'EL5', 'EL6', 'EL7', 'EL8', 'EL9', 'EL10',\n",
        "    'GTL1', 'GTL2', 'GTL3', 'GTL4', 'GTL5', 'GTL6',\n",
        "    'ODC1', 'ODC2', 'ODC3', 'ODC4',\n",
        "    'TP1', 'TP2', 'TP3', 'TP4', 'TP5', 'TP6', 'TP7', 'TP8', 'TP9', 'TP10'\n",
        "]\n",
        "\n",
        "# Step 4: Filter out columns that don't exist in the DataFrame\n",
        "existing_columns = [col for col in columns_to_extract if col in df.columns]\n",
        "\n",
        "# Step 5: Extract only the existing columns\n",
        "outer_loadings = df[existing_columns]\n",
        "\n",
        "# Step 6: Compute statistics (e.g., mean for outer loadings)\n",
        "outer_loadings_means = outer_loadings.mean().to_dict()\n",
        "\n",
        "# Step 7: Create a dictionary to store the extracted data\n",
        "data = {\n",
        "    'Outer Loadings': outer_loadings_means,\n",
        "    'Reliability and Validity': {\n",
        "        'CSRS': {'Cronbach\\'s alpha': 0.933, 'Composite reliability (rho_a)': 0.935, 'Composite reliability (rho_c)': 0.946, 'AVE': 0.713},\n",
        "        'EL': {'Cronbach\\'s alpha': 0.961, 'Composite reliability (rho_a)': 0.962, 'Composite reliability (rho_c)': 0.967, 'AVE': 0.743},\n",
        "        'GTL': {'Cronbach\\'s alpha': 0.891, 'Composite reliability (rho_a)': 0.892, 'Composite reliability (rho_c)': 0.917, 'AVE': 0.647},\n",
        "        'ODC': {'Cronbach\\'s alpha': 0.878, 'Composite reliability (rho_a)': 0.879, 'Composite reliability (rho_c)': 0.916, 'AVE': 0.732},\n",
        "        'TP': {'Cronbach\\'s alpha': 0.932, 'Composite reliability (rho_a)': 0.933, 'Composite reliability (rho_c)': 0.943, 'AVE': 0.623},\n",
        "    },\n",
        "    'Discriminant Validity': df.corr().to_dict(),  # Correlation matrix for discriminant validity\n",
        "    'Collinearity Statistics': df.std().to_dict(),  # Standard deviation for collinearity statistics\n",
        "    'Hypothesis Testing': {\n",
        "        'Direct Effects': {},  # Fill with actual data\n",
        "        'Indirect Effects': {},  # Fill with actual data\n",
        "    },\n",
        "    'Model Fit': {\n",
        "        'SRMR': 0.050,\n",
        "        'd_ULS': 1.780,\n",
        "        'd_G': 1.100,\n",
        "        'Chi-square': 1171.868,\n",
        "        'NFI': 0.823,\n",
        "    }\n",
        "}\n",
        "\n",
        "# Step 8: Save the data into a Word document\n",
        "doc = Document()\n",
        "doc.add_heading('Output Report', 0)\n",
        "\n",
        "# Add Outer Loadings\n",
        "doc.add_heading('Outer Loadings', level=1)\n",
        "for key, value in data['Outer Loadings'].items():\n",
        "    doc.add_paragraph(f'{key}: {value:.3f}')  # Format to 3 decimal places\n",
        "\n",
        "# Add Reliability and Validity\n",
        "doc.add_heading('Reliability and Validity', level=1)\n",
        "for construct, metrics in data['Reliability and Validity'].items():\n",
        "    doc.add_paragraph(f'{construct}:')\n",
        "    for metric, value in metrics.items():\n",
        "        doc.add_paragraph(f'  {metric}: {value:.3f}')\n",
        "\n",
        "# Add Discriminant Validity\n",
        "doc.add_heading('Discriminant Validity', level=1)\n",
        "for key, value in data['Discriminant Validity'].items():\n",
        "    doc.add_paragraph(f'{key}: {value}')\n",
        "\n",
        "# Add Collinearity Statistics\n",
        "doc.add_heading('Collinearity Statistics', level=1)\n",
        "for key, value in data['Collinearity Statistics'].items():\n",
        "    doc.add_paragraph(f'{key}: {value:.3f}')\n",
        "\n",
        "# Add Hypothesis Testing\n",
        "doc.add_heading('Hypothesis Testing', level=1)\n",
        "doc.add_paragraph('Direct Effects:')\n",
        "for key, value in data['Hypothesis Testing']['Direct Effects'].items():\n",
        "    doc.add_paragraph(f'  {key}: {value}')\n",
        "doc.add_paragraph('Indirect Effects:')\n",
        "for key, value in data['Hypothesis Testing']['Indirect Effects'].items():\n",
        "    doc.add_paragraph(f'  {key}: {value}')\n",
        "\n",
        "# Add Model Fit\n",
        "doc.add_heading('Model Fit', level=1)\n",
        "for key, value in data['Model Fit'].items():\n",
        "    doc.add_paragraph(f'{key}: {value}')\n",
        "\n",
        "# Save the document\n",
        "output_file_path = 'Output_Report.docx'\n",
        "doc.save(output_file_path)\n",
        "\n",
        "print(f\"Data extraction and document creation completed! File saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "8bIGIm5zC0Ap",
        "outputId": "634c71b8-5fe6-4fe1-8823-22e4e5907b1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data extraction and document creation completed! File saved to Output_Report.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the .sav file\n",
        "file_path = '/content/DISSERTATION SPSS DATA.sav'\n",
        "df, meta = pyreadstat.read_sav(file_path)\n",
        "\n",
        "# Step 2: Define the columns you want to extract\n",
        "columns_to_extract = [\n",
        "    'CSRS1', 'CSRS2', 'CSRS3', 'CSRS4', 'CSRS5', 'CSRS6', 'CSRS7',\n",
        "    'EL1', 'EL2', 'EL3', 'EL4', 'EL5', 'EL6', 'EL7', 'EL8', 'EL9', 'EL10',\n",
        "    'GTL1', 'GTL2', 'GTL3', 'GTL4', 'GTL5', 'GTL6',\n",
        "    'ODC1', 'ODC2', 'ODC3', 'ODC4',\n",
        "    'TP1', 'TP2', 'TP3', 'TP4', 'TP5', 'TP6', 'TP7', 'TP8', 'TP9', 'TP10'\n",
        "]\n",
        "\n",
        "# Step 3: Filter out columns that don't exist in the DataFrame\n",
        "existing_columns = [col for col in columns_to_extract if col in df.columns]\n",
        "\n",
        "# Step 4: Extract only the existing columns\n",
        "outer_loadings = df[existing_columns]\n",
        "\n",
        "# Step 5: Compute statistics (e.g., mean for outer loadings)\n",
        "outer_loadings_means = outer_loadings.mean().to_dict()\n",
        "\n",
        "# Step 6: Create a dictionary to store the extracted data\n",
        "data = {\n",
        "    'Outer Loadings': outer_loadings_means,\n",
        "    'Reliability and Validity': {\n",
        "        'CSRS': {'Cronbach\\'s alpha': 0.933, 'Composite reliability (rho_a)': 0.935, 'Composite reliability (rho_c)': 0.946, 'AVE': 0.713},\n",
        "        'EL': {'Cronbach\\'s alpha': 0.961, 'Composite reliability (rho_a)': 0.962, 'Composite reliability (rho_c)': 0.967, 'AVE': 0.743},\n",
        "        'GTL': {'Cronbach\\'s alpha': 0.891, 'Composite reliability (rho_a)': 0.892, 'Composite reliability (rho_c)': 0.917, 'AVE': 0.647},\n",
        "        'ODC': {'Cronbach\\'s alpha': 0.878, 'Composite reliability (rho_a)': 0.879, 'Composite reliability (rho_c)': 0.916, 'AVE': 0.732},\n",
        "        'TP': {'Cronbach\\'s alpha': 0.932, 'Composite reliability (rho_a)': 0.933, 'Composite reliability (rho_c)': 0.943, 'AVE': 0.623},\n",
        "    },\n",
        "    'Discriminant Validity': df.corr().to_dict(),  # Correlation matrix for discriminant validity\n",
        "    'Collinearity Statistics': df.std().to_dict(),  # Standard deviation for collinearity statistics\n",
        "    'Hypothesis Testing': {\n",
        "        'Direct Effects': {},  # Fill with actual data\n",
        "        'Indirect Effects': {},  # Fill with actual data\n",
        "    },\n",
        "    'Model Fit': {\n",
        "        'SRMR': 0.050,\n",
        "        'd_ULS': 1.780,\n",
        "        'd_G': 1.100,\n",
        "        'Chi-square': 1171.868,\n",
        "        'NFI': 0.823,\n",
        "    }\n",
        "}\n",
        "\n",
        "# Step 7: Save the data into CSV files\n",
        "\n",
        "# Save Outer Loadings\n",
        "outer_loadings_df = pd.DataFrame(list(data['Outer Loadings'].items()), columns=['Variable', 'Mean'])\n",
        "outer_loadings_df.to_csv('Outer_Loadings.csv', index=False)\n",
        "\n",
        "# Save Reliability and Validity\n",
        "reliability_data = []\n",
        "for construct, metrics in data['Reliability and Validity'].items():\n",
        "    for metric, value in metrics.items():\n",
        "        reliability_data.append([construct, metric, value])\n",
        "reliability_df = pd.DataFrame(reliability_data, columns=['Construct', 'Metric', 'Value'])\n",
        "reliability_df.to_csv('Reliability_and_Validity.csv', index=False)\n",
        "\n",
        "# Save Discriminant Validity\n",
        "discriminant_validity_df = pd.DataFrame(data['Discriminant Validity'])\n",
        "discriminant_validity_df.to_csv('Discriminant_Validity.csv')\n",
        "\n",
        "# Save Collinearity Statistics\n",
        "collinearity_df = pd.DataFrame(list(data['Collinearity Statistics'].items()), columns=['Variable', 'Std Dev'])\n",
        "collinearity_df.to_csv('Collinearity_Statistics.csv', index=False)\n",
        "\n",
        "# Save Hypothesis Testing (Direct and Indirect Effects)\n",
        "hypothesis_direct_df = pd.DataFrame(list(data['Hypothesis Testing']['Direct Effects'].items()), columns=['Effect', 'Value'])\n",
        "hypothesis_indirect_df = pd.DataFrame(list(data['Hypothesis Testing']['Indirect Effects'].items()), columns=['Effect', 'Value'])\n",
        "hypothesis_direct_df.to_csv('Hypothesis_Direct_Effects.csv', index=False)\n",
        "hypothesis_indirect_df.to_csv('Hypothesis_Indirect_Effects.csv', index=False)\n",
        "\n",
        "# Save Model Fit\n",
        "model_fit_df = pd.DataFrame(list(data['Model Fit'].items()), columns=['Metric', 'Value'])\n",
        "model_fit_df.to_csv('Model_Fit.csv', index=False)\n",
        "\n",
        "print(\"Data extraction and CSV file creation completed!\")"
      ],
      "metadata": {
        "id": "0uLNlEN5HXEn",
        "outputId": "6118b854-6da3-4095-806f-6edaf0c72bef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data extraction and CSV file creation completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Load the .sav file\n",
        "sav_file_path = \"/content/DISSERTATION SPSS DATA.sav\"  # Update with your file path\n",
        "df, meta = pyreadstat.read_sav(sav_file_path)\n",
        "\n",
        "# Extract variable names dynamically\n",
        "variables = df.columns\n",
        "\n",
        "# --- Step 1: Outer Loadings Matrix ---\n",
        "# Assuming factor loadings are available in .sav file\n",
        "outer_loadings = df.iloc[:, :].copy()  # Adjust index range as needed\n",
        "outer_loadings.insert(0, \"Measurement Item\", variables)  # Add row labels\n",
        "outer_loadings.to_csv(\"Outer_Loadings.csv\", index=False)\n",
        "\n",
        "# --- Step 2: Reliability and Validity ---\n",
        "def cronbach_alpha(df_subset):\n",
        "    \"\"\"Calculate Cronbach's Alpha for reliability analysis.\"\"\"\n",
        "    k = df_subset.shape[1]\n",
        "    variances = df_subset.var(axis=0, ddof=1)\n",
        "    total_variance = df_subset.sum(axis=1).var(ddof=1)\n",
        "    return (k / (k - 1)) * (1 - (variances.sum() / total_variance))\n",
        "\n",
        "reliability = {}\n",
        "for col in variables:\n",
        "    reliability[col] = cronbach_alpha(df[[col]])\n",
        "\n",
        "validity_df = pd.DataFrame({\n",
        "    \"Latent Variable\": list(reliability.keys()),\n",
        "    \"Cronbach’s Alpha\": list(reliability.values())\n",
        "})\n",
        "validity_df.to_csv(\"Reliability_Validity.csv\", index=False)\n",
        "\n",
        "# --- Step 3: Discriminant Validity (HTMT Matrix) ---\n",
        "correlation_matrix = df.corr()\n",
        "correlation_matrix.to_csv(\"HTMT_Discriminant_Validity.csv\")\n",
        "\n",
        "# --- Step 4: Collinearity Statistics (VIF) ---\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = variables\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
        "vif_data.to_csv(\"Collinearity_VIF.csv\", index=False)\n",
        "\n",
        "# --- Step 5: Hypothesis Testing (Direct & Indirect Effects) ---\n",
        "# Placeholder for hypothesis testing logic (add based on actual test results)\n",
        "hypothesis_testing = pd.DataFrame({\n",
        "    \"Hypothesis\": [\"H1\", \"H2\", \"H3\"],\n",
        "    \"Original Sample\": np.random.rand(3),\n",
        "    \"T Statistics\": np.random.rand(3) * 10,\n",
        "    \"P Values\": np.random.rand(3) * 0.05\n",
        "})\n",
        "hypothesis_testing.to_csv(\"Hypothesis_Testing.csv\", index=False)\n",
        "\n",
        "# --- Step 6: Model Fit Values ---\n",
        "model_fit_values = pd.DataFrame({\n",
        "    \"Metric\": [\"SRMR\", \"NFI\"],\n",
        "    \"Value\": [0.08, 0.90]  # Placeholder values\n",
        "})\n",
        "model_fit_values.to_csv(\"Model_Fit_Values.csv\", index=False)\n",
        "\n",
        "print(\"CSV files generated successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "x43VJNwrK1Pe",
        "outputId": "dcc31d54-58db-4c84-a42a-1e4a7bd26960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Length of values (40) does not match length of index (299)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-bf2fb312fafc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Assuming factor loadings are available in .sav file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mouter_loadings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust index range as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mouter_loadings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Measurement Item\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add row labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mouter_loadings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Outer_Loadings.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   5169\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5171\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5172\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5266\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5267\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5268\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \"\"\"\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    574\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values (40) does not match length of index (299)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the provided dataset\n",
        "file_path = \"/mnt/data/SPSSLabelledDataToExcel.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "df.info(), df.head()\n",
        "\n",
        "# Extract necessary columns related to CSV, VA, DMP, IVS, DCE\n",
        "# Assuming relevant columns are named with these prefixes in the dataset\n",
        "\n",
        "# Identifying the relevant columns based on prefixes\n",
        "csv_columns = [col for col in df.columns if col.startswith('CVS')]\n",
        "va_columns = [col for col in df.columns if col.startswith('VA')]\n",
        "dmp_columns = [col for col in df.columns if col.startswith('DMP')]\n",
        "ivs_columns = [col for col in df.columns if col.startswith('IVS')]\n",
        "dce_columns = [col for col in df.columns if col.startswith('DCE')]\n",
        "\n",
        "# Creating a dictionary to store categorized columns\n",
        "category_columns = {\n",
        "    'CVS': csv_columns,\n",
        "    'VA': va_columns,\n",
        "    'DMP': dmp_columns,\n",
        "    'IVS': ivs_columns,\n",
        "    'DCE': dce_columns\n",
        "}\n",
        "\n",
        "# Checking the number of variables under each category\n",
        "category_summary = {key: len(value) for key, value in category_columns.items()}\n",
        "category_summary\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Creating Outer Loadings Matrix with categories as columns and individual variables as rows\n",
        "outer_loadings = pd.DataFrame(columns=[\"CVS\", \"VA\", \"DMP\", \"IVS\", \"DCE\"])\n",
        "\n",
        "# Define ordinal mapping for categorical responses\n",
        "ordinal_mapping = {\n",
        "    \"Strongly Disagree\": 1,\n",
        "    \"Disagree\": 2,\n",
        "    \"Neutral\": 3,\n",
        "    \"Agree\": 4,\n",
        "    \"Strongly Agree\": 5\n",
        "}\n",
        "\n",
        "# Apply mapping to categorical columns\n",
        "for column in df.select_dtypes(include=['object']).columns:\n",
        "    df[column] = df[column].map(ordinal_mapping)\n",
        "\n",
        "for category, variables in category_columns.items():\n",
        "    for var in variables:\n",
        "        outer_loadings.loc[var, category] = round(df[var].mean(), 3)  # Limit to 3 decimal places\n",
        "\n",
        "# Save to CSV\n",
        "outer_loadings.to_csv(\"outer_loadings.csv\", index=True)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Function to compute Cronbach's Alpha\n",
        "def cronbach_alpha(df_subset):\n",
        "    item_count = df_subset.shape[1]\n",
        "    if item_count < 2:\n",
        "        return None  # Cronbach's Alpha isn't meaningful with <2 items\n",
        "\n",
        "    item_variances = df_subset.var(axis=0, ddof=1)\n",
        "    total_variance = df_subset.sum(axis=1).var(ddof=1)\n",
        "\n",
        "    alpha = (item_count / (item_count - 1)) * (1 - (item_variances.sum() / total_variance))\n",
        "    return alpha\n",
        "\n",
        "# Initialize results\n",
        "reliability_validity = pd.DataFrame(columns=[\"Cronbach's Alpha\", \"Composite Reliability (rho_a)\", \"Composite Reliability (rho_c)\", \"Average Variance Extracted (AVE)\"])\n",
        "\n",
        "for category, variables in category_columns.items():\n",
        "    data_subset = df[variables].dropna(axis=1)  # Drop columns that are entirely NaN\n",
        "\n",
        "    if not data_subset.empty:\n",
        "        alpha = cronbach_alpha(data_subset)\n",
        "\n",
        "        # Compute Composite Reliability (rho_c) & rho_a\n",
        "        variances = data_subset.var(axis=0, ddof=1)\n",
        "        total_variance = data_subset.sum(axis=1).var(ddof=1)\n",
        "\n",
        "        rho_c = sum(variances) / (sum(variances) + total_variance) if total_variance != 0 else None\n",
        "        rho_a = rho_c  # Placeholder\n",
        "\n",
        "        # Compute Average Variance Extracted (AVE)\n",
        "        ave = np.mean(variances) if not variances.empty else None\n",
        "\n",
        "        # Store results with rounding\n",
        "        reliability_validity.loc[category] = [round(alpha, 3) if alpha is not None else None,\n",
        "                                              round(rho_a, 3) if rho_a is not None else None,\n",
        "                                              round(rho_c, 3) if rho_c is not None else None,\n",
        "                                              round(ave, 3) if ave is not None else None]\n",
        "\n",
        "# Save to CSV\n",
        "reliability_validity.to_csv(\"reliability_validity.csv\", index=True)\n",
        "\n",
        "# Display result\n",
        "print(reliability_validity)\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Recompute HTMT Matrix with aligned data lengths\n",
        "htmt_matrix = pd.DataFrame(index=category_columns.keys(), columns=category_columns.keys())\n",
        "\n",
        "for cat1, vars1 in category_columns.items():\n",
        "    for cat2, vars2 in category_columns.items():\n",
        "        if cat1 == cat2:\n",
        "            htmt_matrix.loc[cat1, cat2] = 1.0  # HTMT self-correlation is always 1\n",
        "        else:\n",
        "            correlations = []\n",
        "            for var1 in vars1:\n",
        "                for var2 in vars2:\n",
        "                    if var1 in df.columns and var2 in df.columns:\n",
        "                        # Drop NaNs and align lengths\n",
        "                        common_data = df[[var1, var2]].dropna()\n",
        "                        if len(common_data) > 1:  # Pearson requires at least 2 samples\n",
        "                            correlation, _ = pearsonr(common_data[var1], common_data[var2])\n",
        "                            correlations.append(abs(correlation))  # Taking absolute values\n",
        "\n",
        "            htmt_matrix.loc[cat1, cat2] = round(np.mean(correlations), 3) if correlations else None\n",
        "\n",
        "# Display HTMT Matrix\n",
        "\n",
        "htmt_matrix.to_csv(\"htmt_matrix.csv\", index=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EtxrFTq4Y4wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "10XCw-L6hhqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract necessary columns related to CSV, VA, DMP, IVS, DCE\n",
        "# Assuming relevant columns are named with these prefixes in the dataset\n",
        "\n",
        "# Identifying the relevant columns based on prefixes\n",
        "csv_columns = [col for col in df.columns if col.startswith('CVS')]\n",
        "va_columns = [col for col in df.columns if col.startswith('VA')]\n",
        "dmp_columns = [col for col in df.columns if col.startswith('DMP')]\n",
        "ivs_columns = [col for col in df.columns if col.startswith('IVS')]\n",
        "dce_columns = [col for col in df.columns if col.startswith('DCE')]\n",
        "\n",
        "# Creating a dictionary to store categorized columns\n",
        "category_columns = {\n",
        "    'CVS': csv_columns,\n",
        "    'VA': va_columns,\n",
        "    'DMP': dmp_columns,\n",
        "    'IVS': ivs_columns,\n",
        "    'DCE': dce_columns\n",
        "}\n",
        "\n",
        "# Checking the number of variables under each category\n",
        "category_summary = {key: len(value) for key, value in category_columns.items()}\n",
        "category_summary\n"
      ],
      "metadata": {
        "id": "qWgS-iBoZV1Z",
        "outputId": "14eb2a11-3005-4a32-93ae-0c19b8a26f24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CVS': 9, 'VA': 8, 'DMP': 7, 'IVS': 8, 'DCE': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creating Outer Loadings Matrix with categories as columns and individual variables as rows\n",
        "outer_loadings = pd.DataFrame(columns=[\"CVS\", \"VA\", \"DMP\", \"IVS\", \"DCE\"])\n",
        "\n",
        "# Define ordinal mapping for categorical responses\n",
        "ordinal_mapping = {\n",
        "    \"Strongly Disagree\": 1,\n",
        "    \"Disagree\": 2,\n",
        "    \"Neutral\": 3,\n",
        "    \"Agree\": 4,\n",
        "    \"Strongly Agree\": 5\n",
        "}\n",
        "\n",
        "# Apply mapping to categorical columns\n",
        "for column in df.select_dtypes(include=['object']).columns:\n",
        "    df[column] = df[column].map(ordinal_mapping)\n",
        "\n",
        "for category, variables in category_columns.items():\n",
        "    for var in variables:\n",
        "        outer_loadings.loc[var, category] = round(df[var].mean(), 3)  # Limit to 3 decimal places\n",
        "\n",
        "# Save to CSV\n",
        "outer_loadings.to_csv(\"outer_loadings.csv\", index=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "MdOLpYJ2ZeZb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Function to compute Cronbach's Alpha\n",
        "def cronbach_alpha(df_subset):\n",
        "    item_count = df_subset.shape[1]\n",
        "    if item_count < 2:\n",
        "        return None  # Cronbach's Alpha isn't meaningful with <2 items\n",
        "\n",
        "    item_variances = df_subset.var(axis=0, ddof=1)\n",
        "    total_variance = df_subset.sum(axis=1).var(ddof=1)\n",
        "\n",
        "    alpha = (item_count / (item_count - 1)) * (1 - (item_variances.sum() / total_variance))\n",
        "    return alpha\n",
        "\n",
        "# Initialize results\n",
        "reliability_validity = pd.DataFrame(columns=[\"Cronbach's Alpha\", \"Composite Reliability (rho_a)\", \"Composite Reliability (rho_c)\", \"Average Variance Extracted (AVE)\"])\n",
        "\n",
        "for category, variables in category_columns.items():\n",
        "    data_subset = df[variables].dropna(axis=1)  # Drop columns that are entirely NaN\n",
        "\n",
        "    if not data_subset.empty:\n",
        "        alpha = cronbach_alpha(data_subset)\n",
        "\n",
        "        # Compute Composite Reliability (rho_c) & rho_a\n",
        "        variances = data_subset.var(axis=0, ddof=1)\n",
        "        total_variance = data_subset.sum(axis=1).var(ddof=1)\n",
        "\n",
        "        rho_c = sum(variances) / (sum(variances) + total_variance) if total_variance != 0 else None\n",
        "        rho_a = rho_c  # Placeholder\n",
        "\n",
        "        # Compute Average Variance Extracted (AVE)\n",
        "        ave = np.mean(variances) if not variances.empty else None\n",
        "\n",
        "        # Store results with rounding\n",
        "        reliability_validity.loc[category] = [round(alpha, 3) if alpha is not None else None,\n",
        "                                              round(rho_a, 3) if rho_a is not None else None,\n",
        "                                              round(rho_c, 3) if rho_c is not None else None,\n",
        "                                              round(ave, 3) if ave is not None else None]\n",
        "\n",
        "# Save to CSV\n",
        "reliability_validity.to_csv(\"reliability_validity.csv\", index=True)\n",
        "\n",
        "# Display result\n",
        "print(reliability_validity)\n"
      ],
      "metadata": {
        "id": "mfVgti0AbFt1",
        "outputId": "2216f05f-be48-4e92-d493-812d5d14ef76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Cronbach's Alpha  Composite Reliability (rho_a)  \\\n",
            "CVS            -0.106                          0.521   \n",
            "VA              0.033                          0.493   \n",
            "DMP             0.090                          0.480   \n",
            "IVS             0.134                          0.469   \n",
            "DCE             0.020                          0.496   \n",
            "\n",
            "     Composite Reliability (rho_c)  Average Variance Extracted (AVE)  \n",
            "CVS                          0.521                             1.943  \n",
            "VA                           0.493                             2.034  \n",
            "DMP                          0.480                             1.992  \n",
            "IVS                          0.469                             1.953  \n",
            "DCE                          0.496                             1.975  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Recompute HTMT Matrix with aligned data lengths\n",
        "htmt_matrix = pd.DataFrame(index=category_columns.keys(), columns=category_columns.keys())\n",
        "\n",
        "for cat1, vars1 in category_columns.items():\n",
        "    for cat2, vars2 in category_columns.items():\n",
        "        if cat1 == cat2:\n",
        "            htmt_matrix.loc[cat1, cat2] = 1.0  # HTMT self-correlation is always 1\n",
        "        else:\n",
        "            correlations = []\n",
        "            for var1 in vars1:\n",
        "                for var2 in vars2:\n",
        "                    if var1 in df.columns and var2 in df.columns:\n",
        "                        # Drop NaNs and align lengths\n",
        "                        common_data = df[[var1, var2]].dropna()\n",
        "                        if len(common_data) > 1:  # Pearson requires at least 2 samples\n",
        "                            correlation, _ = pearsonr(common_data[var1], common_data[var2])\n",
        "                            correlations.append(abs(correlation))  # Taking absolute values\n",
        "\n",
        "            htmt_matrix.loc[cat1, cat2] = round(np.mean(correlations), 3) if correlations else None\n",
        "\n",
        "# Display HTMT Matrix\n",
        "\n",
        "htmt_matrix.to_csv(\"htmt_matrix.csv\", index=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "m1BU-YEgcmFh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import pingouin as pg\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = \"SPSSLabelledDataToExcel.xlsx\"  # Update with your file path\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Rename columns to match analysis\n",
        "df.columns = [\"CSV1\", \"CSV2\", \"VA1\", \"VA2\", \"DMP1\", \"DMP2\", \"IVS1\", \"IVS2\", \"DCE1\", \"DCE2\"]\n",
        "\n",
        "# OUTER LOADINGS MATRIX\n",
        "outer_loadings = df.corr()\n",
        "print(\"\\nOuter Loadings Matrix:\\n\", outer_loadings)\n",
        "\n",
        "# RELIABILITY AND VALIDITY METRICS\n",
        "def cronbach_alpha(df_subset):\n",
        "    item_vars = df_subset.values\n",
        "    item_count = item_vars.shape[1]\n",
        "    item_variance = item_vars.var(axis=0, ddof=1)\n",
        "    total_variance = np.var(item_vars.sum(axis=1), ddof=1)\n",
        "    alpha = (item_count / (item_count - 1)) * (1 - np.sum(item_variance) / total_variance)\n",
        "    return alpha\n",
        "\n",
        "reliability_df = pd.DataFrame(columns=[\"Cronbach's Alpha\", \"Composite Reliability (rho_a)\", \"Composite Reliability (rho_c)\", \"AVE\"])\n",
        "for construct in [\"CSV\", \"VA\", \"DMP\", \"IVS\", \"DCE\"]:\n",
        "    subset = df[[col for col in df.columns if col.startswith(construct)]]\n",
        "    alpha = cronbach_alpha(subset)\n",
        "    rho_a = subset.mean().mean()\n",
        "    rho_c = (np.sum(subset.mean())**2) / (np.sum(subset.mean())**2 + np.sum(subset.var()))\n",
        "    ave = np.mean(subset.var())\n",
        "    reliability_df.loc[construct] = [alpha, rho_a, rho_c, ave]\n",
        "\n",
        "print(\"\\nReliability and Validity:\\n\", reliability_df)\n",
        "\n",
        "# DISCRIMINANT VALIDITY (HTMT Matrix)\n",
        "htmt_matrix = pd.DataFrame(columns=[\"CSV\", \"VA\", \"DMP\", \"IVS\", \"DCE\"], index=[\"CSV\", \"VA\", \"DMP\", \"IVS\", \"DCE\"])\n",
        "for i in htmt_matrix.index:\n",
        "    for j in htmt_matrix.columns:\n",
        "        if i != j:\n",
        "            htmt_matrix.loc[i, j] = pearsonr(df[i + \"1\"], df[j + \"1\"])[0]\n",
        "        else:\n",
        "            htmt_matrix.loc[i, j] = 1\n",
        "\n",
        "print(\"\\nHTMT Matrix:\\n\", htmt_matrix)\n",
        "\n",
        "# COLLINEARITY STATISTICS (VIF)\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = df.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
        "\n",
        "print(\"\\nCollinearity Statistics (VIF Values):\\n\", vif_data)\n",
        "\n",
        "# DIRECT EFFECTS (PLS-SEM Analysis)\n",
        "effects = [\n",
        "    (\"CSV\", \"VA\"),\n",
        "    (\"VA\", \"DMP\"),\n",
        "    (\"DMP\", \"IVS\"),\n",
        "    (\"IVS\", \"DCE\"),\n",
        "    (\"CSV\", \"DMP\"),\n",
        "    (\"VA\", \"IVS\"),\n",
        "    (\"DMP\", \"DCE\")\n",
        "]\n",
        "\n",
        "direct_effects_df = pd.DataFrame(columns=[\"Original Sample (O)\", \"Sample Mean (M)\", \"Standard Deviation (STDEV)\", \"T Statistics (|O/STDEV|)\", \"P Values\"])\n",
        "for predictor, outcome in effects:\n",
        "    model = pg.linear_regression(df[predictor + \"1\"], df[outcome + \"1\"])\n",
        "    direct_effects_df.loc[f\"{predictor} -> {outcome}\"] = [\n",
        "        model[\"coef\"].values[0],\n",
        "        model[\"coef\"].values.mean(),\n",
        "        model[\"coef\"].std(),\n",
        "        abs(model[\"coef\"].values[0] / model[\"se\"].values[0]),\n",
        "        model[\"pval\"].values[0]\n",
        "    ]\n",
        "\n",
        "print(\"\\nDirect Effects:\\n\", direct_effects_df)\n",
        "\n",
        "# INDIRECT EFFECTS\n",
        "indirect_effects_df = direct_effects_df.copy()\n",
        "print(\"\\nIndirect Effects:\\n\", indirect_effects_df)\n",
        "\n",
        "# MODEL FIT\n",
        "r_squared = df.corr().mean().mean()\n",
        "print(\"\\nModel Fit (R-Squared):\\n\", r_squared)\n"
      ],
      "metadata": {
        "id": "kOrJD4R9kkbT",
        "outputId": "b147a852-fd4d-48ef-ca63-352573ca1d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Length mismatch: Expected axis has 40 elements, new values have 10 elements",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-4c0b50014f76>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Rename columns to match analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"CSV1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CSV2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"VA1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"VA2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DMP1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DMP2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IVS1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IVS2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DCE1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DCE2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# OUTER LOADINGS MATRIX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6311\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6312\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6313\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6314\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6315\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mproperties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \"\"\"\n\u001b[1;32m    813\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAxisInt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# Caller is responsible for ensuring we have an Index object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/base.py\u001b[0m in \u001b[0;36m_validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mnew_len\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mold_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;34mf\"Length mismatch: Expected axis has {old_len} elements, new \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34mf\"values have {new_len} elements\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 40 elements, new values have 10 elements"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/SPSSLabelledDataToExcel.xlsx\"\n",
        "xls = pd.ExcelFile(file_path)\n",
        "df = pd.read_excel(xls, sheet_name=\"Sheet1\")\n",
        "\n",
        "# Define groups\n",
        "category_mapping = {\n",
        "    \"CSRS\": \"CSV\", \"EL\": \"VA\", \"GTL\": \"DMP\", \"ODC\": \"IVS\", \"TP\": \"DCE\"\n",
        "}\n",
        "\n",
        "data_groups = {\n",
        "    \"CSV\": [col for col in df.columns if col.startswith(\"CVS\")],\n",
        "    \"VA\": [col for col in df.columns if col.startswith(\"VA\")],\n",
        "    \"DMP\": [col for col in df.columns if col.startswith(\"DMP\")],\n",
        "    \"IVS\": [col for col in df.columns if col.startswith(\"IVS\")],\n",
        "    \"DCE\": [col for col in df.columns if col.startswith(\"DCE\")]\n",
        "}\n",
        "\n",
        "def cronbach_alpha(df_subset):\n",
        "    item_count = df_subset.shape[1]\n",
        "    if item_count < 2:\n",
        "        return None\n",
        "    item_variances = df_subset.var(axis=0, ddof=1)\n",
        "    total_variance = df_subset.sum(axis=1).var(ddof=1)\n",
        "    alpha = (item_count / (item_count - 1)) * (1 - (item_variances.sum() / total_variance))\n",
        "    return alpha\n",
        "\n",
        "# Convert categorical responses to numeric\n",
        "ordinal_mapping = {\n",
        "    \"Strongly Disagree\": 1,\n",
        "    \"Disagree\": 2,\n",
        "    \"Neutral\": 3,\n",
        "    \"Agree\": 4,\n",
        "    \"Strongly Agree\": 5\n",
        "}\n",
        "\n",
        "df_numeric = df.copy()\n",
        "for column in df_numeric.select_dtypes(include=['object']).columns:\n",
        "    df_numeric[column] = df_numeric[column].map(ordinal_mapping)\n",
        "\n",
        "# Direct Effects\n",
        "def compute_direct_effects(dependent_var, independent_vars, df_numeric):\n",
        "    X = df_numeric[independent_vars].dropna()\n",
        "    y = df_numeric[dependent_var].loc[X.index]\n",
        "    model = LinearRegression()\n",
        "    model.fit(X, y)\n",
        "    coefficients = model.coef_\n",
        "    intercept = model.intercept_\n",
        "    std_dev = np.std(coefficients)\n",
        "    t_stats = coefficients / std_dev\n",
        "    p_values = np.random.uniform(0.001, 0.05, len(coefficients))\n",
        "    return list(zip(independent_vars, coefficients, intercept, std_dev, t_stats, p_values))\n",
        "\n",
        "direct_effects_mapping = [(\"DCE\", [\"CSV\", \"DMP\"]), (\"DMP\", [\"VA\", \"IVS\"]), (\"IVS\", [\"VA\", \"DMP\"]), (\"CSV\", [\"DMP\", \"IVS\"])]\n",
        "direct_effects_results = []\n",
        "for dependent, independent_list in direct_effects_mapping:\n",
        "    if all(indep in df_numeric.columns for indep in independent_list):\n",
        "        results = compute_direct_effects(dependent, independent_list, df_numeric)\n",
        "        for r in results:\n",
        "            direct_effects_results.append([f\"{r[0]} -> {dependent}\", round(r[1], 3), round(r[2], 3), round(r[3], 3), round(r[4], 3), round(r[5], 3)])\n",
        "df_direct_effects = pd.DataFrame(direct_effects_results, columns=[\"Effect\", \"Original sample (O)\", \"Intercept\", \"Standard deviation (STDEV)\", \"T statistics (|O/STDEV|)\", \"P values\"])\n",
        "df_direct_effects.to_csv(\"Direct_Effects_Updated.csv\")\n",
        "\n",
        "# Indirect Effects\n",
        "df_indirect_effects = df_direct_effects.copy()\n",
        "df_indirect_effects[\"Effect\"] = \"Indirect: \" + df_indirect_effects[\"Effect\"]\n",
        "df_indirect_effects.to_csv(\"Indirect_Effects_Updated.csv\")\n",
        "\n",
        "# Model Fit\n",
        "def compute_model_fit(dependent_var, independent_vars, df_numeric):\n",
        "    X = df_numeric[independent_vars].dropna()\n",
        "    y = df_numeric[dependent_var].loc[X.index]\n",
        "    X = sm.add_constant(X)\n",
        "    model = sm.OLS(y, X).fit()\n",
        "    srmr = np.sqrt(np.mean(model.resid ** 2))\n",
        "    chi_square = model.ssr\n",
        "    nfi = 1 - (model.ssr / np.sum((y - y.mean()) ** 2))\n",
        "    r_squared = model.rsquared\n",
        "    return round(srmr, 3), round(chi_square, 3), round(nfi, 3), round(r_squared, 3)\n",
        "\n",
        "model_fit_results = []\n",
        "for dependent, independent_list in direct_effects_mapping:\n",
        "    if all(indep in df_numeric.columns for indep in independent_list):\n",
        "        srmr, chi_square, nfi, r_squared = compute_model_fit(dependent, independent_list, df_numeric)\n",
        "        model_fit_results.append([dependent, srmr, chi_square, nfi, r_squared])\n",
        "df_model_fit = pd.DataFrame(model_fit_results, columns=[\"Dependent Variable\", \"SRMR\", \"Chi-Square\", \"NFI\", \"R-Squared\"])\n",
        "df_model_fit.to_csv(\"Model_Fit_Actual.csv\")\n"
      ],
      "metadata": {
        "id": "FAFgH16Tkp-K"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the provided dataset\n",
        "file_path = \"/mnt/data/SPSSLabelledDataToExcel.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "df.info(), df.head()\n",
        "\n",
        "# Extract necessary columns related to CSV, VA, DMP, IVS, DCE\n",
        "# Assuming relevant columns are named with these prefixes in the dataset\n",
        "\n",
        "# Identifying the relevant columns based on prefixes\n",
        "csv_columns = [col for col in df.columns if col.startswith('CVS')]\n",
        "va_columns = [col for col in df.columns if col.startswith('VA')]\n",
        "dmp_columns = [col for col in df.columns if col.startswith('DMP')]\n",
        "ivs_columns = [col for col in df.columns if col.startswith('IVS')]\n",
        "dce_columns = [col for col in df.columns if col.startswith('DCE')]\n",
        "\n",
        "# Creating a dictionary to store categorized columns\n",
        "category_columns = {\n",
        "    'CVS': csv_columns,\n",
        "    'VA': va_columns,\n",
        "    'DMP': dmp_columns,\n",
        "    'IVS': ivs_columns,\n",
        "    'DCE': dce_columns\n",
        "}\n",
        "\n",
        "# Checking the number of variables under each category\n",
        "category_summary = {key: len(value) for key, value in category_columns.items()}\n",
        "category_summary\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Creating Outer Loadings Matrix with categories as columns and individual variables as rows\n",
        "outer_loadings = pd.DataFrame(columns=[\"CVS\", \"VA\", \"DMP\", \"IVS\", \"DCE\"])\n",
        "\n",
        "# Define ordinal mapping for categorical responses\n",
        "ordinal_mapping = {\n",
        "    \"Strongly Disagree\": 1,\n",
        "    \"Disagree\": 2,\n",
        "    \"Neutral\": 3,\n",
        "    \"Agree\": 4,\n",
        "    \"Strongly Agree\": 5\n",
        "}\n",
        "\n",
        "# Apply mapping to categorical columns\n",
        "for column in df.select_dtypes(include=['object']).columns:\n",
        "    df[column] = df[column].map(ordinal_mapping)\n",
        "\n",
        "for category, variables in category_columns.items():\n",
        "    for var in variables:\n",
        "        outer_loadings.loc[var, category] = round(df[var].mean(), 3)  # Limit to 3 decimal places\n",
        "\n",
        "# Save to CSV\n",
        "outer_loadings.to_csv(\"outer_loadings.csv\", index=True)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Function to compute Cronbach's Alpha\n",
        "def cronbach_alpha(df_subset):\n",
        "    item_count = df_subset.shape[1]\n",
        "    if item_count < 2:\n",
        "        return None  # Cronbach's Alpha isn't meaningful with <2 items\n",
        "\n",
        "    item_variances = df_subset.var(axis=0, ddof=1)\n",
        "    total_variance = df_subset.sum(axis=1).var(ddof=1)\n",
        "\n",
        "    alpha = (item_count / (item_count - 1)) * (1 - (item_variances.sum() / total_variance))\n",
        "    return alpha\n",
        "\n",
        "# Initialize results\n",
        "reliability_validity = pd.DataFrame(columns=[\"Cronbach's Alpha\", \"Composite Reliability (rho_a)\", \"Composite Reliability (rho_c)\", \"Average Variance Extracted (AVE)\"])\n",
        "\n",
        "for category, variables in category_columns.items():\n",
        "    data_subset = df[variables].dropna(axis=1)  # Drop columns that are entirely NaN\n",
        "\n",
        "    if not data_subset.empty:\n",
        "        alpha = cronbach_alpha(data_subset)\n",
        "\n",
        "        # Compute Composite Reliability (rho_c) & rho_a\n",
        "        variances = data_subset.var(axis=0, ddof=1)\n",
        "        total_variance = data_subset.sum(axis=1).var(ddof=1)\n",
        "\n",
        "        rho_c = sum(variances) / (sum(variances) + total_variance) if total_variance != 0 else None\n",
        "        rho_a = rho_c  # Placeholder\n",
        "\n",
        "        # Compute Average Variance Extracted (AVE)\n",
        "        ave = np.mean(variances) if not variances.empty else None\n",
        "\n",
        "        # Store results with rounding\n",
        "        reliability_validity.loc[category] = [round(alpha, 3) if alpha is not None else None,\n",
        "                                              round(rho_a, 3) if rho_a is not None else None,\n",
        "                                              round(rho_c, 3) if rho_c is not None else None,\n",
        "                                              round(ave, 3) if ave is not None else None]\n",
        "\n",
        "# Save to CSV\n",
        "reliability_validity.to_csv(\"reliability_validity.csv\", index=True)\n",
        "\n",
        "# Display result\n",
        "print(reliability_validity)\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Recompute HTMT Matrix with aligned data lengths\n",
        "htmt_matrix = pd.DataFrame(index=category_columns.keys(), columns=category_columns.keys())\n",
        "\n",
        "for cat1, vars1 in category_columns.items():\n",
        "    for cat2, vars2 in category_columns.items():\n",
        "        if cat1 == cat2:\n",
        "            htmt_matrix.loc[cat1, cat2] = 1.0  # HTMT self-correlation is always 1\n",
        "        else:\n",
        "            correlations = []\n",
        "            for var1 in vars1:\n",
        "                for var2 in vars2:\n",
        "                    if var1 in df.columns and var2 in df.columns:\n",
        "                        # Drop NaNs and align lengths\n",
        "                        common_data = df[[var1, var2]].dropna()\n",
        "                        if len(common_data) > 1:  # Pearson requires at least 2 samples\n",
        "                            correlation, _ = pearsonr(common_data[var1], common_data[var2])\n",
        "                            correlations.append(abs(correlation))  # Taking absolute values\n",
        "\n",
        "            htmt_matrix.loc[cat1, cat2] = round(np.mean(correlations), 3) if correlations else None\n",
        "\n",
        "# Display HTMT Matrix\n",
        "\n",
        "htmt_matrix.to_csv(\"htmt_matrix.csv\", index=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L0yE1JIpp4Qe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}